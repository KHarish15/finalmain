version: 2.1

jobs:
  test-and-analyze:
    docker:
      - image: cimg/python:3.10
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            pip install requests pytest pytest-cov pytest-html
            pip install coverage
      - run:
          name: Create comprehensive test suite
          command: |
            # Create a more comprehensive test script
            cat > test_suite.py << 'EOF'
            import requests
            import json
            import os
            import sys
            from datetime import datetime
            import subprocess
            
            def run_unit_tests():
                """Run unit tests and collect results"""
                try:
                    # Create a simple test file
                    with open('test_example.py', 'w') as f:
                        f.write('''
            def test_addition():
                assert 2 + 2 == 4
                
            def test_string():
                assert "hello" + " world" == "hello world"
                
            def test_failure():
                assert 1 == 2  # This will fail
                
            def test_list():
                assert [1, 2, 3] == [1, 2, 3]
            ''')
                    
                    # Run pytest with coverage
                    result = subprocess.run([
                        'python', '-m', 'pytest', 
                        'test_example.py', 
                        '--cov=.', 
                        '--cov-report=html',
                        '--cov-report=term-missing',
                        '-v'
                    ], capture_output=True, text=True)
                    
                    return {
                        'exit_code': result.returncode,
                        'stdout': result.stdout,
                        'stderr': result.stderr,
                        'coverage': 'Coverage report generated in htmlcov/'
                    }
                except Exception as e:
                    return {'error': str(e)}
            
            def analyze_with_ai(test_results):
                """Send test results to AI for analysis"""
                try:
                    ai_payload = {
                        'test_results': {
                            'status': 'completed',
                            'passed': test_results.get('passed', 0),
                            'failed': test_results.get('failed', 0),
                            'coverage': test_results.get('coverage', 'N/A'),
                            'logs': test_results.get('stdout', ''),
                            'errors': test_results.get('stderr', ''),
                            'timestamp': datetime.now().isoformat(),
                            'pipeline_info': {
                                'branch': os.getenv('CIRCLE_BRANCH', 'unknown'),
                                'commit': os.getenv('CIRCLE_SHA1', 'unknown'),
                                'build_number': os.getenv('CIRCLE_BUILD_NUM', 'unknown')
                            }
                        }
                    }
                    
                    response = requests.post(
                        'https://backend-az2r.onrender.com/analyze-logs',
                        json=ai_payload,
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        return response.json()
                    else:
                        return {'error': f'AI API returned {response.status_code}'}
                        
                except Exception as e:
                    return {'error': f'AI analysis failed: {str(e)}'}
            
            def save_to_confluence(ai_analysis, test_results):
                """Save results to Confluence"""
                try:
                    confluence_data = {
                        'space_key': 'TEST',
                        'page_title': f'Test Results - Build {os.getenv("CIRCLE_BUILD_NUM", "unknown")}',
                        'content': f'''
            ## Test Results Summary - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            
            ### Build Information
            - **Branch**: {os.getenv('CIRCLE_BRANCH', 'unknown')}
            - **Commit**: {os.getenv('CIRCLE_SHA1', 'unknown')[:8]}
            - **Build Number**: {os.getenv('CIRCLE_BUILD_NUM', 'unknown')}
            
            ### Test Results
            - **Status**: {'✅ PASSED' if test_results.get('exit_code') == 0 else '❌ FAILED'}
            - **Passed**: {test_results.get('passed', 0)}
            - **Failed**: {test_results.get('failed', 0)}
            - **Coverage**: {test_results.get('coverage', 'N/A')}
            
            ### AI Analysis
            {ai_analysis.get('analysis', 'No analysis available')}
            
            ### Test Logs
            ```
            {test_results.get('stdout', 'No logs available')}
            ```
            
            ---
            *Generated automatically by CircleCI with Agentic AI integration*
            '''
                    }
                    
                    response = requests.post(
                        'https://backend-az2r.onrender.com/save-to-confluence',
                        json=confluence_data,
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        print("✅ Posted to Confluence successfully")
                        return True
                    else:
                        print(f"❌ Failed to post to Confluence: {response.status_code}")
                        return False
                        
                except Exception as e:
                    print(f"❌ Failed to post to Confluence: {e}")
                    return False
            
            # Main execution
            print("🚀 Starting comprehensive test execution...")
            
            # Run tests
            test_results = run_unit_tests()
            
            # Parse test results
            if 'error' not in test_results:
                # Count passed/failed from pytest output
                output = test_results.get('stdout', '')
                passed = len([line for line in output.split('\n') if 'PASSED' in line])
                failed = len([line for line in output.split('\n') if 'FAILED' in line])
                
                test_results['passed'] = passed
                test_results['failed'] = failed
                
                print(f"📊 Test Results: {passed} passed, {failed} failed")
            else:
                print(f"❌ Test execution failed: {test_results['error']}")
            
            # AI Analysis
            print("🤖 Sending results to AI for analysis...")
            ai_analysis = analyze_with_ai(test_results)
            
            if 'error' not in ai_analysis:
                print("✅ AI analysis completed")
                print("📝 AI Analysis Preview:")
                analysis_text = ai_analysis.get('analysis', '')
                print(analysis_text[:200] + "..." if len(analysis_text) > 200 else analysis_text)
            else:
                print(f"❌ AI analysis failed: {ai_analysis['error']}")
            
            # Save to Confluence
            print("📄 Saving results to Confluence...")
            save_to_confluence(ai_analysis, test_results)
            
            print("🎉 Test and analysis pipeline completed!")
            
            # Exit with test result code
            sys.exit(test_results.get('exit_code', 1))
            EOF
      - run:
          name: Execute comprehensive test suite
          command: python test_suite.py
      - store_artifacts:
          path: test_suite.py
          destination: test-artifacts
      - store_artifacts:
          path: htmlcov/
          destination: coverage-reports
      - store_artifacts:
          path: test_example.py
          destination: test-files

  ai-enhanced-testing:
    docker:
      - image: cimg/python:3.10
    steps:
      - checkout
      - run:
          name: Install AI testing dependencies
          command: |
            pip install requests pytest-mock pytest-asyncio
            pip install openai anthropic
      - run:
          name: Create AI-powered test generator
          command: |
            cat > ai_test_generator.py << 'EOF'
            import requests
            import json
            import os
            
            def generate_ai_tests():
                """Generate additional tests using AI"""
                try:
                    # Read existing code
                    with open('test_example.py', 'r') as f:
                        existing_code = f.read()
                    
                    # Ask AI to generate additional test cases
                    ai_prompt = {
                        'space_key': 'TEST',
                        'code_page_title': 'Test Example Code',
                        'instruction': f'''
                        Based on this existing test code:
                        {existing_code}
                        
                        Generate 3 additional comprehensive test cases that would improve coverage.
                        Focus on edge cases, error conditions, and boundary testing.
                        Return only the test functions, no explanations.
                        ''',
                        'target_language': 'python'
                    }
                    
                    response = requests.post(
                        'https://backend-az2r.onrender.com/code-assistant',
                        json=ai_prompt,
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        ai_tests = response.json().get('generated_code', '')
                        
                        # Save AI-generated tests
                        with open('ai_generated_tests.py', 'w') as f:
                            f.write(ai_tests)
                        
                        print("✅ AI-generated tests created")
                        return ai_tests
                    else:
                        print(f"❌ Failed to generate AI tests: {response.status_code}")
                        return None
                        
                except Exception as e:
                    print(f"❌ AI test generation failed: {e}")
                    return None
            
            if __name__ == "__main__":
                generate_ai_tests()
            EOF
      - run:
          name: Generate AI-powered tests
          command: python ai_test_generator.py
      - store_artifacts:
          path: ai_generated_tests.py
          destination: ai-tests

workflows:
  version: 2
  comprehensive-test-workflow:
    jobs:
      - test-and-analyze
      - ai-enhanced-testing:
          requires:
            - test-and-analyze 